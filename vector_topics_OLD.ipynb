{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load speech file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle file\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"speeches.pickle\", \"rb\")\n",
    "speech_df = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>convention</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016</td>\n",
       "      <td>Thank you all for the great convention that we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convention</td>\n",
       "      <td>Robert Dole</td>\n",
       "      <td>1996</td>\n",
       "      <td>The folks in Hollywood would be happy to know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convention</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2000</td>\n",
       "      <td>Thank you. Thank you for this honor. [,],Thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convention</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>2004</td>\n",
       "      <td>When I said those words 4 years ago, none of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convention</td>\n",
       "      <td>John McCain</td>\n",
       "      <td>2008</td>\n",
       "      <td>Tonight, I have a privilege given few American...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type          speaker  date  \\\n",
       "0  convention  Hillary Clinton  2016   \n",
       "1  convention      Robert Dole  1996   \n",
       "2  convention   George W. Bush  2000   \n",
       "3  convention   George W. Bush  2004   \n",
       "4  convention      John McCain  2008   \n",
       "\n",
       "                                              speech  \n",
       "0  Thank you all for the great convention that we...  \n",
       "1  The folks in Hollywood would be happy to know ...  \n",
       "2  Thank you. Thank you for this honor. [,],Thank...  \n",
       "3  When I said those words 4 years ago, none of u...  \n",
       "4  Tonight, I have a privilege given few American...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_text = speech_df['speech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citizens united states 183\n",
      "      ending june 30 177\n",
      "government united states 175\n",
      "    year ending june 168\n",
      "people united states 161\n",
      "    last fiscal year 145\n",
      "  fiscal year ending 144\n",
      "last session congress 141\n",
      " last annual message 138\n",
      "president united states 132\n",
      "united states america 116\n",
      " united states great 104\n",
      "  part united states 100\n",
      "states great britain 89\n",
      "congress last session 82\n",
      "    next fiscal year 76\n",
      "report secretary war 76\n",
      " present fiscal year 75\n",
      "report secretary treasury 69\n",
      "    year ending 30th 68\n",
      " current fiscal year 67\n",
      "united states government 66\n",
      "report secretary navy 66\n",
      "       ended june 30 65\n",
      "     year ended june 64\n",
      "constitution united states 62\n",
      "    fiscal year 1947 62\n",
      "interstate commerce commission 60\n",
      "        world war ii 59\n",
      "  bank united states 59\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "stop = set(stop)\n",
    "\n",
    "n = 3\n",
    "for doc in speech_text:\n",
    "    words = TextBlob(doc.lower()).words  # tokenize words\n",
    "    words = [w for w in words if w not in stop]   \n",
    "    bigrams = ngrams(words, n)\n",
    "    counter += Counter(bigrams)\n",
    "\n",
    "for phrase, count in counter.most_common(30):\n",
    "    print('%20s %i' % (\" \".join(phrase), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "lda = decomposition.LatentDirichletAllocation(\n",
    "    n_components=n_topics, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(lda, count_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\"\n",
    ")\n",
    "\n",
    "t_vectorizer.fit(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_counts = t_vectorizer.transform(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 30\n",
    "\n",
    "nmf = decomposition.NMF(\n",
    "    n_components=n_topics,\n",
    "    max_iter=5\n",
    ")\n",
    "\n",
    "nmf.fit(t_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(nmf, t_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
